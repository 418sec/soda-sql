{"0": {
    "doc": "5 min tutorial",
    "title": "5 min tutorial",
    "content": "# 5 min tutorial If at any time during this tutorial you get stuck, speak up in our [GitHub Discussions forum](https://github.com/sodadata/soda-sql/discussions/). ### 1\\) Check your CLI installation Open a command line and enter `soda` to verify if the soda-sql command line tool is installed correctly. If you don't get this output, check out our [Installation guide]({% link getting-started/installation.md %}). ```shell $ soda Usage: soda [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: create Creates a new project directory and prepares credentials in your... init Initializes scan.yml files based on profiling tables found in the... scan Computes all measurements and runs all tests on one table. ``` ### 2) Set up an example warehouse In this tutorial we'll use PostgreSQL as our data warehouse. Note that Soda SQL also supports Snowflake, AWS Athena, GCP BigQuery, AWS Redshift and others. #### 2.1) Start PostgreSQL as your warehouse To get you going we've included the steps required to setup a pre-configured PostgreSQL container, but you can also choose to use your own PostgreSQL installation. If so, make sure to create a `sodasql` database and an associated `sodasql` user which doesn't require a password. _Command:_ ```shell $ docker run --name soda_sql_tutorial_db --rm -d \\ -p 5432:5432 \\ -v soda_sql_tutorial_postgres:/var/lib/postgresql/data:rw \\ -e POSTGRES_USER=sodasql \\ -e POSTGRES_DB=sodasql \\ -e POSTGRES_HOST_AUTH_METHOD=trust \\ postgres:9.6.17-alpine ``` > As soon as you're done with the tutorial you can use the following commands to clean up the created container and volume: > ```shell > $ docker stop soda_sql_tutorial_db > $ docker volume rm soda_sql_tutorial_postgres > ``` #### 2.2\\) Load example data into your warehouse Use the following command to load [example data](https://github.com/sodadata/soda-sql/blob/main/tests/demo/demodata.sql) into your PostgreSQL tutorial database: _Command:_ ```shell docker exec soda_sql_tutorial_db \\ sh -c \"wget -qO - https://raw.githubusercontent.com/sodadata/soda-sql/main/tests/demo/demodata.sql | psql -U sodasql -d sodasql\" ``` _Command console output:_ ```shell DROP TABLE CREATE TABLE INSERT 0 6 INSERT 0 8 INSERT 0 9 INSERT 0 8 INSERT 0 10 INSERT 0 12 INSERT 0 12 ``` ### 3\\) Create a warehouse directory With our database up-and-running it's time to create our warehouse configuration. In this tutorial we will name our warehouse directory `soda_sql_tutorial` and we'll use the `soda` CLI tool to create the initial directory and `warehouse.yml`. The `warehouse.yml` file which will be created by the command below will include connection details to use the PostgreSQL database we've just set up. The command will also create and store the credentials in `~/.soda/env_vars.yml` _Command:_ ```shell soda create -d sodasql -u sodasql ./soda_sql_tutorial postgres ``` _Command console output:_ ``` | Soda CLI version 2.0.0 beta | Creating warehouse directory ./soda_sql_tutorial ... | Creating warehouse configuration file ./soda_sql_tutorial/warehouse.yml ... | Creating /Users/tom/.soda/env_vars.yml with example env vars in section soda_sql_tutorial | Review warehouse.yml by running command | cat ./soda_sql_tutorial/warehouse.yml | Review section soda_sql_tutorial in ~/.soda/env_vars.yml by running command | cat ~/.soda/env_vars.yml | Then run | soda init ./soda_sql_tutorial ``` The `soda create` command will only create and append configuration files. It will never overwrite or delete existing files so you can safely run the command multiple times, or against an existing directory. Next, review the 2 files that have been created: * `cat ./soda_sql_tutorial/warehouse.yml` * `cat ~/.soda/env_vars.yml` You can continue without changing anything. Check out the [warehouse.yml]({% link documentation/warehouse.md %}) or [env_vars.yml]({% link documentation/cli.md %}#env-vars) documentation to learn more about these files. ### 4\\) Initialize table scan.yml files Now our warehouse is configured it's time to initialize it with a `scan.yml` for each table. We can run the `soda init` command to automatically generate a `scan.yml` for each table in our PostgreSQL warehouse: _Command:_ ```shell soda init ./soda_sql_tutorial ``` _Command console output:_ ``` | Soda CLI version 2.0.0 beta | Initializing ./soda_sql_tutorial ... | Querying warehouse for tables | Executing SQL query: SELECT table_name FROM information_schema.tables WHERE lower(table_schema)='public' | SQL took 0:00:00.005413 | Creating table directory ./soda_sql_tutorial/demodata | Creating ./soda_sql_tutorial/demodata/scan.yml ... | Next run 'soda scan ./soda_sql_tutorial demodata' to calculate measurements and run tests ``` ### 5\\) Review the generated scan.yml files Each `scan.yml` will contain the metric and test instructions used by `soda scan`. By default `soda init` will create a `scan.yml` file with some good defaults, but feel free to modify the generated configurations to fit your needs. > Head over to the [Scan Documentation]({% link documentation/scan.md %}) for more in-depth information about the `scan.yml` file. _Command:_ ```shell cat ./soda_sql_tutorial/demodata/scan.yml ``` _Command console output:_ ```shell table_name: demodata metrics: - row_count - missing_count - missing_percentage - values_count - values_percentage - valid_count - valid_percentage - invalid_count - invalid_percentage - min - max - avg - sum - min_length - max_length - avg_length tests: rows: row_count > 0 ``` ### 6\\) Run a scan With your warehouse directory created and initialized it's time to start scanning. Each scan will collect the configured (`scan.yml`) metrics and run the defined tests against them. To run your first scan on the `demodata` table simply run: _Command:_ ```shell soda scan ./soda_sql_tutorial demodata ``` _Command console output:_ ```shell | Soda CLI version 2.0.0 beta | Scanning demodata in ./soda_sql_tutorial ... | Environment variable POSTGRES_PASSWORD is not set | Executing SQL query: SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE lower(table_name) = 'demodata' AND table_catalog = 'sodasql' AND table_schema = 'public' | SQL took 0:00:00.029199 | 6 columns: | id character varying | name character varying | size integer | date date | feepct character varying | country character varying | Query measurement: schema = id character varying, name character varying, size integer, date date, feepct character varying, country character varying | Executing SQL query: SELECT COUNT(*), COUNT(id), MIN(LENGTH(id)), MAX(LENGTH(id)), COUNT(name), MIN(LENGTH(name)), MAX(LENGTH(name)), COUNT(size), ... | missing_count(country) = 0 | values_percentage(country) = 100.0 | All good. 38 measurements computed. No tests failed. ``` ### 7\\) Next steps Congrats! You've just completed all steps required to get you going with `soda-sql`. [Post a quick note letting us know what you like or dislike.](https://github.com/sodadata/soda-sql/discussions/new) Next we suggest you to take a look at some further in-depth documentation which will help you to integrate `soda-sql` into your own project. * See [Tests]({% link documentation/tests.md %}) to add tests. * See [SQL Metrics]({% link documentation/sql_metrics.md %}) to add a custom SQL query as your metric. * See [Orchestrate scans]({% link documentation/orchestrate_scans.md %}) to add scans to your data pipeline. ",
    "url": "/soda-sql/getting-started/5_min_tutorial.html",
    "relUrl": "/getting-started/5_min_tutorial.html"
  },"1": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "# Soda SQL Data testing and monitoring for SQL accessible data. {% comment %} > This is an overview, the full documentation is available at [docs.soda.io/soda-sql](https://docs.soda.io/soda-sql/). {% endcomment %} **What does Soda SQL do?** Soda SQL allows you to * Stop your pipeline when bad data is detected * Extract metrics and column profiles through super efficient SQL * Full control over metrics and queries through declarative config files **Why Soda SQL?** To protect against silent data issues for the consumers of your data, it's best-practice to profile and test your data: * as it lands in your warehouse, * after every important data processing step * right before consumption. This way you will prevent delivery of bad data to downstream consumers. You will spend less time firefighting and gain a better reputation. **How does Soda SQL work?** Soda SQL is a Command Line Interface (CLI) and a Python library to measure and test your data using SQL. As input, Soda SQL uses YAML configuration files that include: * SQL connection details * What metrics to compute * What tests to run on the measurements Based on those configuration files, Soda SQL will perform scans. A scan performs all measurements and runs all tests associated with one table. Typically a scan is executed after new data has arrived. All soda-sql configuration files can be checked into your version control system as part of your pipeline code. > Want to try Soda SQL? Head over to our ['5 minute tutorial'](https://docs.soda.io/soda-sql/getting-started/5_min_tutorial.html) and get started straight away! **Show me the money** Simple metrics and tests can be configured in YAML configuration files called `scan.yml`. An example of the contents of such a file: ```yaml metrics: - row_count - missing_count - missing_percentage - values_count - values_percentage - valid_count - valid_percentage - invalid_count - invalid_percentage - min - max - avg - sum - min_length - max_length - avg_length columns: ID: metrics: - distinct - duplicate_count valid_format: uuid tests: duplicates: duplicate_count == 0 CATEGORY: missing_values: - N/A - No category tests: missing: missing_percentage 5000 ``` Based on these configuration files, Soda SQL will scan your data each time new data arrived like this: ```bash $ soda scan ./soda/metrics my_warehouse my_dataset Soda 1.0 scan for dataset my_dataset on prod my_warehouse | SELECT column_name, data_type, is_nullable | FROM information_schema.columns | WHERE lower(table_name) = 'customers' | AND table_catalog = 'datasource.database' | AND table_schema = 'datasource.schema' - 0.256 seconds Found 4 columns: ID, NAME, CREATE_DATE, COUNTRY | SELECT | COUNT(*), | COUNT(CASE WHEN ID IS NULL THEN 1 END), | COUNT(CASE WHEN ID IS NOT NULL AND ID regexp '\\b[0-9a-f]{8}\\b-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-\\b[0-9a-f]{12}\\b' THEN 1 END), | MIN(LENGTH(ID)), | AVG(LENGTH(ID)), | MAX(LENGTH(ID)), | FROM customers - 0.557 seconds row_count : 23543 missing : 23 invalid : 0 min_length: 9 avg_length: 9 max_length: 9 ...more queries... 47 measurements computed 23 tests executed All is good. No tests failed. Scan took 23.307 seconds ``` The next step is to add Soda SQL scans in your favorite data pipeline orchestration solution like: * Airflow * AWS Glue * Prefect * Dagster * Fivetran * Matillion * Luigi If you like the goals of this project, encourage us! Star [sodadata/soda-sql on Github](https://github.com/sodadata/soda-sql). > Next, head over to our ['5 minute tutorial'](https://docs.soda.io/soda-sql/getting-started/5_min_tutorial) and get your first project going! ",
    "url": "/soda-sql/",
    "relUrl": "/"
  },"2": {
    "doc": "CLI",
    "title": "CLI",
    "content": "# Soda's Command Line Interface (CLI) > See our [Installation guide]({% link getting-started/installation.md %}) on how to install the `soda` command. The soda command line is mostly a tool to help you get started with your Soda SQL configuration files. To see the list of available commands, enter `soda` in your terminal: ``` soda Usage: soda [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: create Creates a new warehouse directory and prepares credentials in your... init Finds tables in the warehouse and based on the contents, creates... scan Computes all measurements and runs all tests on one table. ``` | Command | Description | ------- | ----------- | `soda create ...` | Creates a new warehouse directory and prepares credentials in your ~/.soda/env_vars.yml Nothing will be overwritten or removed, only added if it does not exist yet. | `soda init ...` | Finds tables in the warehouse and based on the contents, creates initial scan.yml files | `soda scan ...` | Computes all measurements and runs all tests on one table. Exit code 0 means all tests passed. Non zero exit code means tests have failed or an exception occurred. If the project has a Soda cloud account configured, measurements and test results will be uploaded | To learn about the parameters, use the command line help: * `soda create --help` * `soda init --help` * `soda scan --help` # Env vars To keep your `warehouse.yml` configuration files free of credentials, soda-sql supports to reference to environment variables by using the `env_vars(SOME_ENV_VAR)` format. The `soda` CLI uses a convenient mechanism to load environment variables from your local user home directory. Each `soda` CLI command which reads a warehouse configuration will also read the corresponding environment variables specified in your `~/.soda/env_vars.yml` file. Example `~/.soda/env_vars.yml` ```yaml my_project_postgres: SNOWFLAKE_USERNAME: someotherexampleusername SNOWFLAKE_PASSWORD: someotherexamplepassword some_other_soda_project: POSTGRES_USERNAME: myexampleusername POSTGRES_PASSWORD: myexamplepassword ``` The `soda create` command will assist in creating and prepopulating the environment variables section in your `~/.soda/env_vars.yml` file. ",
    "url": "/soda-sql/documentation/cli.html",
    "relUrl": "/documentation/cli.html"
  },"3": {
    "doc": "Community",
    "title": "Community",
    "content": "# Community Soda SQL is open source software distributed under the Apache License v2. We hope you like it. Please encourage us by [starring sodadata/soda-sql on Github](https://github.com/sodadata/soda-sql). Get involved, cause we believe Soda SQL is going to be an important part of many data platforms and you have a perspective to contribute. ## GitHub Use [our GitHub's discussions](https://github.com/sodadata/soda-sql/discussions) forum to get in touch with the community. Don't hesitate to: * [Post a question](https://github.com/sodadata/soda-sql/discussions/new) * [Post a problem](https://github.com/sodadata/soda-sql/issues/new) * [Share your feedback](https://github.com/sodadata/soda-sql/discussions/new) * [Suggest an improvement](https://github.com/sodadata/soda-sql/discussions/new) ## Contributing We are very open to any kind of contribution. To start a contribution it's good to, [post your idea on the discussion forum](https://github.com/sodadata/soda-sql/discussions/new). This way the whole community is up to date and can share their thoughts and --if needed-- help you with pointers to get going. ",
    "url": "/soda-sql/community.html",
    "relUrl": "/community.html"
  },"4": {
    "doc": "Concepts",
    "title": "Concepts",
    "content": "# Concepts | Concept | Description | ----------- | ----------- | Warehouse | Any SQL-engine or DB accessible over SQL queries that contains the tables you want to test and monitor | Warehouse directory | The top directory in the Soda SQL recommended folder structure containing the warehouse.yml and the table directories | Table | A SQL table | Table directory | A sub directory of a warehouse directory containing the scan.yml and sql metrics for that table | Column | A SQL column | Metric | A metric is property of a the data like Eg the row count of a table or the minimum value of a column | Default metric | A default metric is a metric that Soda SQL already knows how to compute through SQL | SQL metric | A metric that is computed with a user defined SQL query | Scan | One Soda SQL computation to extract the configured metrics and tests from a table using SQL queries | Measurement | One value for a metric obtained during a scan | Test | A Python expression that checks if certain expected conditions | ",
    "url": "/soda-sql/documentation/concepts.html",
    "relUrl": "/documentation/concepts.html"
  },"5": {
    "doc": "Documentation",
    "title": "Documentation",
    "content": " ",
    "url": "/soda-sql/documentation/",
    "relUrl": "/documentation/"
  },"6": {
    "doc": "Installation",
    "title": "Installation",
    "content": "# Installation The simplest way to use soda-sql is by using the CLI. This section guides you through the installation steps required to get the `soda` command up and running. > As an alternative to the CLI, you can use the Python Programmatic Interface, which > provides you with a more advance way of using Soda SQL. > See [Programmatic scans]({% link documentation/orchestrate_scans.md %}#programmatic-scans). ## Requirements The soda-sql CLI requires the following dependencies to be installed on your system: - Python >=3.7 <3.9 - postgresql-libs (`libpq-dev` in Debian/Ubuntu, `libpq-devel` in CentOS, `postgresql` on MacOSX) - _Linux only:_ `libssl-dev` and `libffi-dev` (`libffi-devel` and `openssl-devel` in CentOS) To check your version of python, run the `python` command ``` $ python --version Python 3.7.7 ``` If you don't have Python, [install it from Python downloads](https://www.python.org/downloads/) which should also provide you with `pip`. ## Installing soda-sql CLI from PyPI ``` $ pip install soda-sql ``` After the installation finishes head over to the [5 Minute Tutorial]({% link getting-started/5_min_tutorial.md %}) to get started. ",
    "url": "/soda-sql/getting-started/installation.html",
    "relUrl": "/getting-started/installation.html"
  },"7": {
    "doc": "Orchestrate scans",
    "title": "Orchestrate scans",
    "content": "# Orchestrate scans This section explains how to run scans as part of your data pipeline and stop the pipeline when necessary to prevent bad data flowing downstream. Soda SQL is build in such a way that it's easy to run it as a step in your pipeline orchestration. Use your orchestration tool to configure if the soda scan should be blocking the pipeline (for testing) or run in parallel (for monitoring). ## Programmatic scans Here's how to run scans using Python: Programmatic scan execution based on default dir structure: ```python scan_builder = ScanBuilder() scan_builder.read_scan_dir('~/my_warehouse_dir', 'my_table_dir') scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_failures(): print('Scan has test failures, stop the pipeline') ``` Programmatic scan execution reading yaml files by path: ```python scan_builder = ScanBuilder() scan_builder.read_warehouse_yml('./anydir/warehouse.yml') scan_builder.read_scan_yml('./anydir/scan.yml') scan_builder.read_sql_metrics_from_dir('./anydir/') scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_failures(): print('Scan has test failures, stop the pipeline') ``` Programmatic scan execution using dicts: ```python scan_builder = ScanBuilder() scan_builder.warehouse_dict({ 'name': 'my_warehouse_name', 'connection': { 'type': 'snowflake', ... } }) scan_builder.scan_dict({...}) scan_builder.sql_metric_dict({...}) scan_builder.sql_metric_dict({...}) scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_failures(): print('Scan has test failures, stop the pipeline') ``` ## Airflow TODO: describe how to run Soda scans in Airflow. If you're reading this and thinking: \"I want to contribute!\" Great. [Post an note on GitHub](https://github.com/sodadata/soda-sql/discussions/new?title=Contributing%20Airflow) to let others know you're starting on this. ## Other orchestration solutions TODO: describe how to run Soda scans in orchestration tools like * AWS Glue * Prefect * Dagster * Fivetran * Matillion * Luigi If you're reading this and thinking: \"I want to contribute!\" Great. [Post an note on GitHub](https://github.com/sodadata/soda-sql/discussions/new) to let others know you're starting on this. ",
    "url": "/soda-sql/documentation/orchestrate_scans.html",
    "relUrl": "/documentation/orchestrate_scans.html"
  },"8": {
    "doc": "Roadmap and release notes",
    "title": "Roadmap and release notes",
    "content": "# Roadmap and release notes ## Current status Soda SQL is currently in beta. While the codebase is quite young, it does build upon our experience of many field implementations and customer inputs. We have seen many home grown solutions for this problem. With this project, we want to combine our expertise with these efforts and jointly build something that any individual party could not achieve. ## Roadmap Now * Support Postgres (Done) End of January 2021 * Support Snowflake * Support AWS Redshift * Support GCP BigQuery * Support AWS Athena Later * Support Spark SQL * Support Dremio For other SQL engines or feature requests, [create an issue](https://github.com/sodadata/soda-sql/issues/new) ## Release notes TODO ",
    "url": "/soda-sql/documentation/roadmap.html",
    "relUrl": "/documentation/roadmap.html"
  },"9": {
    "doc": "Scan",
    "title": "Scan",
    "content": "# Scan This section explains what a scan does, how it works and how to use `scan.yml` files to configure them. For running scans, see either [the CLI]({% link documentation/cli.md %}) or [Orchestrate scans]({% link documentation/orchestrate_scans.md %}). ## Anatomy of a scan A scan is performed on a table and does the following: * Fetch the column metadata of the table (column name, type and nullable) * Single aggregation query that computes aggregate metrics for multiple columns like e.g. missing, min, max etc * For each column * One query for distinct_count, unique_count and valid_count * One query for mins (list of smallest values) * One query for maxs (list of greatest values) * One query for frequent values * One query for histograms > Note on performance: we have tuned most column queries by using the same Column Table Expression (CTE). The goal is to allow some databases, like eg Snowflake, to be able to cache the results, but we didn't see actual proof of this yet. If you have knowledge on this, [drop us a line in one of the channels]({% link community.md %}). ## Top level scan.yml keys In a `scan.yml` file, you configure which metrics should be computed and which tests should be checked. Top level configuration keys: | Key | Description | Required | --- | ----------- | -------- | table_name | The table name. | Required | metrics | The list of metrics to compute. Column metrics specified here will be computed on each column. | Optional | columns | Optionally add metrics and configurations for specific columns | Optional | time_filter | A SQL expression that will be added to query where clause. Uses [Jinja as template language](https://jinja.palletsprojects.com/). Variables can be passed into the scan. See [Time partitioning]({% link documentation/time_partitioning.md %}) | Optional | mins_maxs_limit | Max number of elements for the mins metric | Optional, default is 5 | frequent_values_limit | Max number of elements for the maxs metric | Optional, default is 5 | sample_percentage | Adds [sampling](https://docs.snowflake.com/en/sql-reference/constructs/sample.html) to limit the number of rows scanned. Only tested on Postgres | Optional | sample_method | For Snowflake, One of { BERNOULLI, ROW, SYSTEM, BLOCK } | Required if sample_percentage is specified | ## Metrics ### Table metrics | Meric | Description | ----- | ------------| row_count | | schema | ### Column metrics | Meric | Description | ----- | ------------| missing_count | | missing_percentage | | values_count | | values_percentage | | valid_count | | valid_percentage | | invalid_count | | invalid_percentage | | min | | max | | avg | | sum | | variance | | stddev | | min_length | | max_length | | avg_length | | distinct | | unique_count | | duplicate_count | | uniqueness | | maxs | | mins | | frequent_values | | histogram | ### Metric categories > Deprecated metric categories are now included in the metrics, but that is probably not a > good idea. We're considering to introduce `metric_categories` as a separate top level element | Meric category | Metrics | -------------- | ------------| missing | missing_countmissing_percentagevalues_countvalues_percentage | validity | valid_countvalid_percentageinvalid_countinvalid_percentage | duplicates | distinctunique_countuniquenessduplicate_count | ### Implied metrics | Any metric in | Implies metrics | ------------- | --------------- | valid_countvalid_percentageinvalid_countinvalid_percentage | missing_countmissing_percentagevalues_countvalues_percentage | missing_countmissing_percentagevalues_countvalues_percentage | row_count | histogram | minmax | ## Column configurations Column configuration keys: | Key | Description | --- | ----------- | metrics | Extra metrics to be computed for this column | tests | Tests to be evaluate for this column | missing_values | Customize what values are considered missing | missing_format | To customize missing values such as whitespace and empty strings | missing_regex | Define your own custom missing values | valid_format | Specifies valid values with a named valid text format | valid_regex | Specifies valid values with a regex | valid_values | Specifies valid values with a list of values | valid_min | Specifies a min value for valid values | valid_max | Specifies a max value for valid values | valid_min_length | Specifies a min length for valid values | valid_max_length | Specifies a max length for valid values | ",
    "url": "/soda-sql/documentation/scan.html",
    "relUrl": "/documentation/scan.html"
  },"10": {
    "doc": "SQL metrics",
    "title": "SQL metrics",
    "content": "# SQL metrics The most simple SQL metric is selecting 1 numeric value. For example: > The name of the field will be used as the name of the metric. So be sure to > provide a meaningful field alias for aggregate functions. Otherwise you won't > be able to properly reference the metrics in the test. ```yaml sql: | SELECT sum(volume) as total_volume_us FROM CUSTOMER_TRANSACTIONS WHERE country = 'US' tests: total_volume_greater_than: total_volume_us > 5000 ``` Multiple select fields are supported as well: ```yaml sql: | SELECT sum(volume) as total_volume_us, min(volume) as min_volume_us, max(volume) as max_volume_us FROM CUSTOMER_TRANSACTIONS WHERE country = 'US' tests: total_volume_greater_than: total_volume_us > 5000 min_volume_greater_than: min_volume_us > 20 max_volume_greater_than: max_volume_us > 100 spread_less_than: max_volume_us - min_volume_us 5000 min_volume_greater_than: min_volume > 20 max_volume_greater_than: max_volume > 100 spread_less_than: max_volume - min_volume < 60 ``` ",
    "url": "/soda-sql/documentation/sql_metrics.html",
    "relUrl": "/documentation/sql_metrics.html"
  },"11": {
    "doc": "Tests",
    "title": "Tests",
    "content": "# Tests Tests are evaluated as part of scans and product test results as part of the scan result. When using the CLI, the exit code will be determined by the test results. Tests are simple Python expressions where the metrics are available as variables. For example: * `min > 25` * `missing_percentage <= 2.5` * `5 <= avg_length and avg_length <= 10` Tests can be specified on 3 different places: * `tests` in scan.yml on top level for testing `row_count` and other table level metrics * `tests` in scan.yml on a column level for testing column metrics * `tests` in user defined SQL metrics yaml files ",
    "url": "/soda-sql/documentation/tests.html",
    "relUrl": "/documentation/tests.html"
  },"12": {
    "doc": "Time partitioning",
    "title": "Time partitioning",
    "content": "# Time partitioning This section explains how to run metrics and tests on a single time partition of the table. Time partitioning requires 1 (or more) columns which reflect the time or date. The goal is to run a scan on a partition of the data which corresponds to a particular time period. Let's use this `CUSTOMER_TRANSACTIONS` table as an example: ```sql CREATE TABLE CUSTOMER_TRANSACTIONS ( ID VARCHAR(255), NAME VARCHAR(255), SIZE INT, DATE DATE, FEEPCT VARCHAR(255), COUNTRY VARCHAR(255) ); ``` The `CUSTOMER_TRANSACTIONS` has a `DATE` column. Each day new customer transaction rows are added. After they are added the goal is to run the scan on the customer transactions of the last day. In the `scan.yml`, add a `time_filter` like this: ```yaml table_name: CUSTOMER_TRANSACTIONS time_filter: \"date = DATE '{{ date }}'\" metrics: ... columns: ... ``` The time filter is added to the SQL queries in the where clause. The `date` can be passed to the scan as a variable on the command line like: > _Note: CLI does not yet support variables. Coming soon. Use programmatic style below_ ``` soda scan -v date=2021-01-12 ./sales_snowflake customer_transactions ``` And programmatically, variables can be passed to a scan like this: ```python scan_builder = ScanBuilder() scan_builder.read_scan_dir('~/my_warehouse_dir', 'my_table_dir') scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_failures(): print('Scan has test failures, stop the pipeline') ``` For time partitioned tables, it makes sense to measure and test on both the time partitions and on the full table. To achieve this, we recommend that you create 2 separate table dirs for it each having a `scan.yml`. It's a good practice to add `_tp` as the suffix to the table directory to indicate it's a \"Time Partitioned\" table configuration. ",
    "url": "/soda-sql/documentation/time_partitioning.html",
    "relUrl": "/documentation/time_partitioning.html"
  },"13": {
    "doc": "Warehouse",
    "title": "Warehouse",
    "content": "# Warehouse A directory with a `warehouse.yml` configuration file is considered a Soda warehouse directory and it implies a directory structure of one directory per table. A warehouse represents a connection to any SQL engine like: Snowflake, Redshift, BigQuery, Athena, Postgres, etc . ## Example Here's an example Soda warehouse directory structure: ``` + sales_snowflake | + warehouse.yml | + customers | + scan.yml | + invoices | + scan.yml | + invoices_without_active_country.yml | + invoices_with_inactive_products.yml ``` `warehouse.yml` contains the name of the warehouse and the connection details (see below) `customers` and `invoices` are table directories, each having a [scan.yml]({% link documentation/scan.md %}) configuration file. `invoices_*.yml` are user defined [SQL metrics]({% link documentation/sql_metrics.md %}) that also get executed when a table scan is performed ## warehouse.yml `warehouse.yml` contains the name of the warehouse and the connection details. We encourage usage of environment variables for credentials and other sensitive information to prevent them from being checked-in into your version control system. For example: ```yaml name: my_project_postgres connection: type: postgres host: localhost username: env_var(POSTGRES_USERNAME) password: env_var(POSTGRES_PASSWORD) database: sodasql schema: public ``` The example above shows you how to each environment variables for credentials. Each environment variable, defined using `env_var(VAR_NAME)`, will automatically be resolved using the `env_vars.yml` file in your home directory. More on this can be found in the [cli.yml documentation]({% link documentation/cli.md %}#env-vars). Each warehouse will require different configuration parameters. See [Warehouse types]({% link documentation/warehouse_types.md %}) to learn how to configure each type of warehouse. > Soon, Soda project files will also include an optional link to a Soda cloud account. A cloud account enables you to push the monitoring results after each scan and share them with other people in your data organisation. ",
    "url": "/soda-sql/documentation/warehouse.html",
    "relUrl": "/documentation/warehouse.html"
  },"14": {
    "doc": "Warehouse types",
    "title": "Warehouse types",
    "content": "# Warehouse types Warehouses are configured as part of a [Soda warehouse configuration file]({% link documentation/warehouse.md %}) This section explains the concrete connection properties for each warehouse type. ## Snowflake Example configuration ```yaml name: my_snowflake_project connection: type: snowflake username: env_var(SNOWFLAKE_USERNAME) password: env_var(SNOWFLAKE_PASSWORD) account: YOUR_SNOWFLAKE_ACCOUNT.eu-west-1 warehouse: YOUR_WAREHOUSE database: YOUR_DATABASE schema: PUBLIC ... ``` | Property | Description | Required | -------- | ----------- | -------- | type | `snowflake` | Required | username | Required | password | Required | account | Eg YOUR_SNOWFLAKE_ACCOUNT.eu-west-1 | Required | warehouse | Required | database | Required | schema | Required | ## AWS Athena Example ```yaml name: my_athena_project connection: type: athena database: sodalite_test access_key_id: env_var(AWS_ACCESS_KEY_ID) secret_access_key: env_var(AWS_SECRET_ACCESS_KEY) role_arn: an optional IAM role arn to be assumed region: eu-west-1 staging_dir: ... ``` ## GCP BigQuery ```yaml name: my_bigquery_project connection: type: bigquery account_info: dataset: sodasql ... ``` ## PostgreSQL ```yaml name: my_postgres_project connection: type: postgres host: localhost username: sodasql password: sodasql database: sodasql schema: public ... ``` ## Redshift ```yaml name: my_redshift_project connection: type: redshift host: username: soda password: database: soda_agent_test schema: public access_key_id: env_var(AWS_ACCESS_KEY_ID) secret_access_key: env_var(AWS_SECRET_ACCESS_KEY) role_arn: an optional IAM role arn to be assumed region: eu-west-1 ... ``` ## Spark SQL Coming soon ",
    "url": "/soda-sql/documentation/warehouse_types.html",
    "relUrl": "/documentation/warehouse_types.html"
  }
}
